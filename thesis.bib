@Article{survey_of_survey,
author={Yin, Hongwei
and Sinnott, Richard O.
and Jayaputera, Glenn T.},
title={A survey of video-based human action recognition in team sports},
journal={Artificial Intelligence Review},
year={2024},
month={9},
day={16},
volume={57},
number={11},
pages={293},
abstract={Over the past few decades, numerous studies have focused on identifying and recognizing human actions using machine learning and computer vision techniques. Video-based human action recognition (HAR) aims to detect actions from video sequences automatically. This can cover simple gestures to complex actions involving multiple people interacting with objects. Actions in team sports exhibit a different nature compared to other sports, since they tend to occur at a faster pace and involve more human-human interactions. As a result, research has typically not focused on the challenges of HAR in team sports. This paper comprehensively summarises HAR-related research and applications with specific focus on team sports such as football (soccer), basketball and Australian rules football. Key datasets used for HAR-related team sports research are explored. Finally, common challenges and future work are discussed, and possible research directions identified.},
issn={1573-7462},
doi={10.1007/s10462-024-10934-9},
url={https://doi.org/10.1007/s10462-024-10934-9}
}

@Inbook{Takebayashi2020,
author="Takebayashi, Ayaka
and Iwahori, Yuji
and Fukui, Shinji
and Little, James J.
and Meng, Lin
and Wang, Aili
and Kijsirikul, Boonserm",
editor="Lee, Roger",
title="Fall Detection of Elderly Persons by Action Recognition Using Data Augmentation and State Transition Diagram",
bookTitle="Applied Computing and Information Technology",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="95--109",
abstract="Because of the increaseTakebayashi, Ayaka of the population of elderly people and the decreasing numberIwahori, Yuji of care workers, there is at present a problem ofFukui, Shinji delay the detection of falls requiring emergency treatment.Little, James J. This paper proposes a method to acquireMeng, Lin posture information of a person from a camera watchingWang, Aili over elderly people, and to recognize the behaviorKijsirikul, Boonserm of the person using Long Short Term Memory. The proposed method detects falls of elderly people automatically using the result of action recognition. Since it is difficult to capture dangerous scenes such as fall of elderly people and to capture a lot of data, this paper proposes a new method that can create a large amount of data necessary for learning from a small amount of data.",
isbn="978-3-030-25217-5",
doi="10.1007/978-3-030-25217-5_8",
url="https://doi.org/10.1007/978-3-030-25217-5_8"
}

@misc{web:pst_trussel,
  author = {PST - Politiets sikkerhetstjeneste},
  title = {Trusselvurdering - økt terrortrussel i Norge
},
  year = {2024},
  month = {Oct},
  day={8},
  note = {Last accessed 13 November 2024},
  url = {https://politietssikkerhetstjeneste.no/alle-artikler/artikler/trusselvurdering---okt-terrortrussel-i-norge/}
}

@article{dataset:something_something,
  author       = {Raghav Goyal and
                  Samira Ebrahimi Kahou and
                  Vincent Michalski and
                  Joanna Materzynska and
                  Susanne Westphal and
                  Heuna Kim and
                  Valentin Haenel and
                  Ingo Fr{\"{u}}nd and
                  Peter Yianilos and
                  Moritz Mueller{-}Freitag and
                  Florian Hoppe and
                  Christian Thurau and
                  Ingo Bax and
                  Roland Memisevic},
  title        = {The "something something" video database for learning and evaluating
                  visual common sense},
  journal      = {CoRR},
  volume       = {abs/1706.04261},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.04261},
  eprinttype    = {arXiv},
  eprint       = {1706.04261},
  timestamp    = {Mon, 13 Aug 2018 16:48:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GoyalKMMWKHFYMH17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{dataset:UCF101,
  author       = {Khurram Soomro and
                  Amir Roshan Zamir and
                  Mubarak Shah},
  title        = {{UCF101:} {A} Dataset of 101 Human Actions Classes From Videos in
                  The Wild},
  journal      = {CoRR},
  volume       = {abs/1212.0402},
  year         = {2012},
  url          = {http://arxiv.org/abs/1212.0402},
  eprinttype    = {arXiv},
  eprint       = {1212.0402},
  timestamp    = {Mon, 13 Aug 2018 16:47:45 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1212-0402.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{dataset:HMDB51,
  author={Kuehne, H. and Jhuang, H. and Garrote, E. and Poggio, T. and Serre, T.},
  booktitle={2011 International Conference on Computer Vision}, 
  title={HMDB: A large video database for human motion recognition}, 
  year={2011},
  volume={},
  number={},
  pages={2556-2563},
  abstract={With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.},
  keywords={Cameras;YouTube;Databases;Training;Visualization;Humans;Motion pictures},
  doi={10.1109/ICCV.2011.6126543},
  ISSN={2380-7504},
  month={Nov},}



@inproceedings{bhogal_human_2023,
	location = {Singapore},
	title = {Human Activity Recognition Using {LSTM} with Feature Extraction Through {CNN}},
	isbn = {978-981-16-9967-2},
	abstract = {Human activity recognition is important for detecting anomalies from videos. The analysis of auspicious activities using videos is increasingly important for security, surveillance, and personal archiving. This research paper has given a model which can recognize activities in random videos. The architecture has been designed by using {BiLSTM} layer which helps to learn a system based on time dependencies. To convert every frame into a featured vector, the pre-trained {GoogLeNet} network has been used. The evaluation has been done by using a public {HMDB}51 data set. The accuracy achieved by using the model is 93.04\% for ten classes and 63.96\% for 51 classes from same data set only. Then, this network is compared with other state-of-the-art method, and it proves to be a better approach for the recognition of activities.},
	pages = {245--255},
	booktitle = {Smart Trends in Computing and Communications},
	publisher = {Springer Nature Singapore},
	author = {Bhogal, Rosepreet Kaur and Devendran, V.},
	editor = {Zhang, Yu-Dong and Senjyu, Tomonobu and So-In, Chakchai and Joshi, Amit},
	date = {2023},
}



@misc{statsbomb_introducing_2021,
	title = {Introducing {On}-{Ball} {Value} ({OBV}) {\textbar} {Hudl} {Statsbomb}},
	url = {https://statsbomb.com/news/introducing-on-ball-value-obv/},
	abstract = {Hudl Statsbomb introduces On-Ball Value (OBV) which can objectively and quantitatively measure the value of events on the pitch.},
	language = {en-US},
	urldate = {2024-12-01},
	journal = {Hudl Statsbomb {\textbar} Data Champions},
	author = {Statsbomb, Hudl},
	month = sep,
	year = {2021},
	file = {Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\F8U8885X\\introducing-on-ball-value-obv.html:text/html},
}

@article{cioppa_soccernet_2023,
	title = {{SoccerNet} 2023 challenges results},
	volume = {27},
	issn = {1460-2687},
	url = {https://doi.org/10.1007/s12283-024-00466-4},
	doi = {10.1007/s12283-024-00466-4},
	abstract = {The SoccerNet 2023 challenges were the third annual video understanding challenges organized by the SoccerNet team. For this third edition, the challenges were composed of seven vision-based tasks split into three main themes. The first theme, broadcast video understanding, is composed of three high-level tasks related to describing events occurring in the video broadcasts: (1) action spotting, focusing on retrieving all timestamps related to global actions in soccer, (2) ball action spotting, focusing on retrieving all timestamps related to the soccer ball change of state, and (3) dense video captioning, focusing on describing the broadcast with natural language and anchored timestamps. The second theme, field understanding, relates to the single task of (4) camera calibration, focusing on retrieving the intrinsic and extrinsic camera parameters from images. The third and last theme, player understanding, is composed of three low-level tasks related to extracting information about the players: (5) re-identification, focusing on retrieving the same players across multiple views, (6) multiple object tracking, focusing on tracking players and the ball through unedited video streams, and (7) jersey number recognition, focusing on recognizing the jersey number of players from tracklets. Compared to the previous editions of the SoccerNet challenges, tasks (2-3-7) are novel, including new annotations and data, task (4) was enhanced with more data and annotations, and task (6) now focuses on end-to-end approaches. Our report indicates performance trends across tasks: (1) Action spotting is nearing saturation, while (2) ball action spotting improved significantly with advanced end-to-end models. (3) Dense video captioning also saw substantial enhancements aligned with Large Language Models advancements. (4) Camera calibration, redefined end-to-end, demonstrated a significant performance boost. In contrast, (5) player re-identification showed only minor improvements, reflecting decreasing interest. The new (6) multiple object tracking task exhibited notable advances, underscoring the maturity of current techniques. (7) Jersey number recognition received the most focus, achieving impressive results. More information on the tasks, challenges, and leaderboards are available on https://www.soccer-net.org. Baselines and development kits can be found on https://github.com/SoccerNet.},
	number = {2},
	journal = {Sports Engineering},
	author = {Cioppa, Anthony and Giancola, Silvio and Somers, Vladimir and Magera, Floriane and Zhou, Xin and Mkhallati, Hassan and Deliège, Adrien and Held, Jan and Hinojosa, Carlos and Mansourian, Amir M. and Miralles, Pierre and Barnich, Olivier and De Vleeschouwer, Christophe and Alahi, Alexandre and Ghanem, Bernard and Van Droogenbroeck, Marc and Kamal, Abdullah and Maglo, Adrien and Clapés, Albert and Abdelaziz, Amr and Xarles, Artur and Orcesi, Astrid and Scott, Atom and Liu, Bin and Lim, Byoungkwon and Chen, Chen and Deuser, Fabian and Yan, Feng and Yu, Fufu and Shitrit, Gal and Wang, Guanshuo and Choi, Gyusik and Kim, Hankyul and Guo, Hao and Fahrudin, Hasby and Koguchi, Hidenari and Ardö, Håkan and Salah, Ibrahim and Yerushalmy, Ido and Muhammad, Iftikar and Uchida, Ikuma and Be’ery, Ishay and Rabarisoa, Jaonary and Lee, Jeongae and Fu, Jiajun and Yin, Jianqin and Xu, Jinghang and Nang, Jongho and Denize, Julien and Li, Junjie and Zhang, Junpei and Kim, Juntae and Synowiec, Kamil and Kobayashi, Kenji and Zhang, Kexin and Habel, Konrad and Nakajima, Kota and Jiao, Licheng and Ma, Lin and Wang, Lizhi and Wang, Luping and Li, Menglong and Zhou, Mengying and Nasr, Mohamed and Abdelwahed, Mohamed and Liashuha, Mykola and Falaleev, Nikolay and Oswald, Norbert and Jia, Qiong and Pham, Quoc-Cuong and Song, Ran and Hérault, Romain and Peng, Rui and Chen, Ruilong and Liu, Ruixuan and Baikulov, Ruslan and Fukushima, Ryuto and Escalera, Sergio and Lee, Seungcheon and Chen, Shimin and Ding, Shouhong and Someya, Taiga and Moeslund, Thomas B. and Li, Tianjiao and Shen, Wei and Zhang, Wei and Li, Wei and Dai, Wei and Luo, Weixin and Zhao, Wending and Zhang, Wenjie and Yang, Xinquan and Ma, Yanbiao and Joo, Yeeun and Zeng, Yingsen and Gan, Yiyang and Zhu, Yongqiang and Zhong, Yujie and Ruan, Zheng and Li, Zhiheng and Huang, Zhijian and Meng, Ziyu},
	month = jul,
	year = {2024},
	pages = {24},
}

@misc{seweryn_survey_2023,
	title = {Survey of {Action} {Recognition}, {Spotting} and {Spatio}-{Temporal} {Localization} in {Soccer} -- {Current} {Trends} and {Research} {Perspectives}},
	url = {http://arxiv.org/abs/2309.12067},
	doi = {10.48550/arXiv.2309.12067},
	abstract = {Action scene understanding in soccer is a challenging task due to the complex and dynamic nature of the game, as well as the interactions between players. This article provides a comprehensive overview of this task divided into action recognition, spotting, and spatio-temporal action localization, with a particular emphasis on the modalities used and multimodal methods. We explore the publicly available data sources and metrics used to evaluate models' performance. The article reviews recent state-of-the-art methods that leverage deep learning techniques and traditional methods. We focus on multimodal methods, which integrate information from multiple sources, such as video and audio data, and also those that represent one source in various ways. The advantages and limitations of methods are discussed, along with their potential for improving the accuracy and robustness of models. Finally, the article highlights some of the open research questions and future directions in the field of soccer action recognition, including the potential for multimodal methods to advance this field. Overall, this survey provides a valuable resource for researchers interested in the field of action scene understanding in soccer.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Seweryn, Karolina and Wróblewska, Anna and Łukasik, Szymon},
	month = sep,
	year = {2023},
	note = {arXiv:2309.12067},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\RKDWPUQN\\Seweryn et al. - 2023 - Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\B4Y4V2I4\\2309.html:text/html},
}

@misc{ren_dino-x_2024,
	title = {{DINO}-{X}: {A} {Unified} {Vision} {Model} for {Open}-{World} {Object} {Detection} and {Understanding}},
	shorttitle = {{DINO}-{X}},
	url = {http://arxiv.org/abs/2411.14347},
	doi = {10.48550/arXiv.2411.14347},
	abstract = {In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, both improving the previous SOTA performance by 5.8 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.},
	urldate = {2024-12-01},
	publisher = {arXiv},
	author = {Ren, Tianhe and Chen, Yihao and Jiang, Qing and Zeng, Zhaoyang and Xiong, Yuda and Liu, Wenlong and Ma, Zhengyu and Shen, Junyi and Gao, Yuan and Jiang, Xiaoke and Chen, Xingyu and Song, Zhuheng and Zhang, Yuhong and Huang, Hongjie and Gao, Han and Liu, Shilong and Zhang, Hao and Li, Feng and Yu, Kent and Zhang, Lei},
	month = nov,
	year = {2024},
	note = {arXiv:2411.14347},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\jofal\\Zotero\\storage\\ADCET2M2\\Ren et al. - 2024 - DINO-X A Unified Vision Model for Open-World Object Detection and Understanding.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\VRDJFWBX\\2411.html:text/html},
}

@article{ravi_sam_nodate,
	title = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
    year = {2024},
    journal = {arXiv},
	language = {en},
	author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollár, Piotr and Feichtenhofer, Christoph},
	file = {PDF:C\:\\Users\\jofal\\Zotero\\storage\\R7J55Z6F\\Ravi et al. - SAM 2 Segment Anything in Images and Videos.pdf:application/pdf},
}

@INPROCEEDINGS{dalal_histogram_of_gradients,
  author={Dalal, N. and Triggs, B.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Histograms of oriented gradients for human detection}, 
  year={2005},
  volume={1},
  number={},
  pages={886-893 vol. 1},
  keywords={Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases},
  doi={10.1109/CVPR.2005.177}}


@InProceedings{dalal_histogram_of_flow,
author="Dalal, Navneet
and Triggs, Bill
and Schmid, Cordelia",
editor="Leonardis, Ale{\v{s}}
and Bischof, Horst
and Pinz, Axel",
title="Human Detection Using Oriented Histograms of Flow and Appearance",
booktitle="Computer Vision -- ECCV 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="428--441",
abstract="Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8{\%} miss rate on our Test Set 1.",
isbn="978-3-540-33835-2"
}

@inproceedings{krizhevsky_alexnet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@INPROCEEDINGS{karpathy_deep_video,
  author={Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Large-Scale Video Classification with Convolutional Neural Networks}, 
  year={2014},
  volume={},
  number={},
  pages={1725-1732},
  keywords={Streaming media;Training;Computer architecture;Spatial resolution;Computational modeling;Feature extraction;convolutional;neural;network;video;classification;action;recognition;large-scale;dataset;sports},
  doi={10.1109/CVPR.2014.223},
  ISSN={1063-6919},
  month=jun,
}

@misc{tran_2_plus_1_convolution,
      title={A Closer Look at Spatiotemporal Convolutions for Action Recognition}, 
      author={Du Tran and Heng Wang and Lorenzo Torresani and Jamie Ray and Yann LeCun and Manohar Paluri},
      year={2018},
      eprint={1711.11248},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1711.11248}, 
}

@article{maheriya__2024_aerial_cnn,
	title = {Insights into aerial intelligence: assessing {CNN}-based algorithms for human action recognition and object detection in diverse environments},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-024-19611-z},
	doi = {10.1007/s11042-024-19611-z},
	abstract = {Today’s era follows a data-driven decision process for large-scale environment analysis. Aerial view-based decision process plays a key role in various domains including surveillance, disaster responses, city planning, and military operations. One of the important and challenging data-driven processes is human action recognition and object detection from the aerial view. The major challenge in aerial view is its notable variance in the scale and perspective of humans and objects. The size of an individual depends on the distance from the camera making object location and human action recognition difficult. In addition, low resolution or poor visibility, occlusion clutter background, and motion blur limit the accurate object detection and localization. CNN’s ability to learn hierarchical representation, take advantage of spatial relationships, manage translation invariance, and leverage transfer learning make it a powerful tool among all for HAR and object detection from an aerial view. We studied applications and progress of aerial view-based human activity recognition (HAR) and object detection (OD) techniques. Various challenges in different environments have been addressed for both HAR and OD examining recent datasets and algorithms spanning seven years. Initially, a categorization of various CNN-based algorithms and their strength and weaknesses are presented for both HAR and OD respectively. Later, a comparative analysis using evaluation matrices and research challenges are discussed in the paper. The comparative study reveals that the self-attention mechanism in CNN performed better among all models but more extensive research is necessary to raise performance standards. A promising future trajectory is outlined for both HAR and OD from an aerial view perspective.},
	journal = {Multimedia Tools and Applications},
	author = {Maheriya, Krunal and Rahevar, Mrugendrasinh and Mewada, Hiren and Parmar, Martin and Patel, Atul},
	month = jun,
	year = {2024},
}

@inproceedings{carreira_2017_i3d_quo_vadis,
	address = {Honolulu, HI},
	title = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
	url = {http://ieeexplore.ieee.org/document/8099985/},
	doi = {10.1109/CVPR.2017.502},
	language = {en},
	urldate = {2024-11-28},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Carreira, Joao and Zisserman, Andrew},
	month = jul,
	year = {2017},
	pages = {4724--4733},
}

@inproceedings{yu_i3d_2023,
author = {Yu, Jie and Lai, Yi and Liu, Ying},
title = {I3D convolutional network algorithm with feature gating},
year = {2024},
isbn = {9798400707674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641584.3641682},
doi = {10.1145/3641584.3641682},
abstract = {In order to effectively solve the persistent problems of low accuracy and high computational complexity in video retrieval, we propose a feature-controlled retrieval algorithm which combines a three-dimensional perception network–Inflated 3-Dimensional (I3D) and a gated recursive unit (GRU) network, and extends the inception module to three dimensions, allowing the formation of three dimensional network structures. The network structure of the three-dimensional module is constructed and gated features are incorporated into it, making it a self-attentive mechanism that is used to improve the accuracy of the system. Finally, the network is completed using soft-max to classify the data sets for two modal inputs, namely color model and optical flow. The respective experimental results show that the network achieves an accuracy of 0.94 on the data sets, thus demonstrating the effectiveness of this method.Additional Keywords and Phrases: I3D network, GRU model, Convolutional Neural Networks, Video retrieval},
booktitle = {Proceedings of the 2023 6th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {663–668},
numpages = {6},
location = {Xiamen, China},
series = {AIPR '23}
}

@misc{vaswani_attention_2017,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}


@inproceedings{kumar_human_2023,
	address = {Cham},
	title = {Human {Activity} {Detection} {Using} {Attention}-{Based} {Deep} {Network}},
	isbn = {978-3-031-25194-8},
	abstract = {Nowadays, digital surveillance devices are widely implemented to collect massive volumes of data indefinitely, necessitating human monitoring to identify various activities. The requirement for smarter surveillance in this era is for artificial intelligence and computer vision technology to automatically identify normal and aberrant actions. We present a long short-term memory (LSTM)-based attention mechanism based on a pre-train convolutional neural network (CNN) that focuses on the most important characteristics in the source frame to distinguish human activities in videos. We employ the DenseNet layers to extract the prominent spatial features from frames. We input these characteristics into an LSTM to learn temporal features in video; after that, an attention technique is used to enhance performance and calculate more high-level selected activity-related patterns. The presented system was evaluated on UCF11 datasets and achieved recognition rates of 97.90\%, demonstrating a substantial advancement over the current state-of-the-art (SOTA) method.},
	booktitle = {Applications of {Computational} {Intelligence} in {Management} \& {Mathematics}},
	publisher = {Springer International Publishing},
	author = {Kumar, Manoj and Biswas, Mantosh},
	editor = {Mishra, Madhusudhan and Kesswani, Nishtha and Brigui, Imene},
	year = {2023},
	pages = {305--315},
}

@article{mahaseni_spotting_2021,
	title = {Spotting {Football} {Events} {Using} {Two}-{Stream} {Convolutional} {Neural} {Network} and {Dilated} {Recurrent} {Neural} {Network}},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104642224&doi=10.1109%2fACCESS.2021.3074831&partnerID=40&md5=fccae982f02ecb3d32d05676980ae2de},
	doi = {10.1109/ACCESS.2021.3074831},
	abstract = {This paper addresses the problem of event detection and localization in long football (soccer) videos. Our key idea is that understanding the long-range dependencies between video frames is imperative for accurate event localization in long football videos. Additionally, proper event detection is not likely for fast movements in football videos without considering mid-range and short-range correlations between neighboring video frames. We argue that event spotting can be considerably improved by considering short-range to long-range frame dependencies in a unified architecture. To model long-range and mid-range dependencies, we propose to use the dilated recurrent neural network (DilatedRNN) with long short-term memory (LSTM) units, grounded on two-stream convolutional neural network (Two-stream CNN) features. While two-stream CNN extracts local spatiotemporal features necessary for fine-level details, the DilatedRNN makes the information obtained from distant frames available for the classifier and spotting algorithms. Evaluating our event spotting algorithm on the largest publicly available benchmark football dataset -SoccerNet- shows an accuracy improvement of 0.8\% - 13.6\% compared to state of the art, and up to 30.1\% accuracy gain in comparison to the baselines. We also investigate the contribution of each neural network component in spotting accuracy through an extensive ablation study.  © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Mahaseni, B. and Faizal, E.R.M. and Raj, R.G.},
	year = {2021},
	keywords = {Deep learning, Convolutional neural networks, Long short-term memory, Classification (of information), Convolution, State of the art, Football, Accuracy Improvement, dilated RNNs, Event localizations, Long-range dependencies, Short-range correlations, Spatio temporal features, sport videos event detection, Spotting algorithms, two-stream CNNs, Unified architecture},
	pages = {61929--61942},
}


@Article{host_handball_2023,
AUTHOR = {Host, Kristina and Pobar, Miran and Ivasic-Kos, Marina},
TITLE = {Analysis of Movement and Activities of Handball Players Using Deep Neural Networks},
JOURNAL = {Journal of Imaging},
VOLUME = {9},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {80},
URL = {https://www.mdpi.com/2313-433X/9/4/80},
PubMedID = {37103231},
ISSN = {2313-433X},
ABSTRACT = {This paper focuses on image and video content analysis of handball scenes and applying deep learning methods for detecting and tracking the players and recognizing their activities. Handball is a team sport of two teams played indoors with the ball with well-defined goals and rules. The game is dynamic, with fourteen players moving quickly throughout the field in different directions, changing positions and roles from defensive to offensive, and performing different techniques and actions. Such dynamic team sports present challenging and demanding scenarios for both the object detector and the tracking algorithms and other computer vision tasks, such as action recognition and localization, with much room for improvement of existing algorithms. The aim of the paper is to explore the computer vision-based solutions for recognizing player actions that can be applied in unconstrained handball scenes with no additional sensors and with modest requirements, allowing a broader adoption of computer vision applications in both professional and amateur settings. This paper presents semi-manual creation of custom handball action dataset based on automatic player detection and tracking, and models for handball action recognition and localization using Inflated 3D Networks (I3D). For the task of player and ball detection, different configurations of You Only Look Once (YOLO) and Mask Region-Based Convolutional Neural Network (Mask R-CNN) models fine-tuned on custom handball datasets are compared to original YOLOv7 model to select the best detector that will be used for tracking-by-detection algorithms. For the player tracking, DeepSORT and Bag of tricks for SORT (BoT SORT) algorithms with Mask R-CNN and YOLO detectors were tested and compared. For the task of action recognition, I3D multi-class model and ensemble of binary I3D models are trained with different input frame lengths and frame selection strategies, and the best solution is proposed for handball action recognition. The obtained action recognition models perform well on the test set with nine handball action classes, with average F1 measures of 0.69 and 0.75 for ensemble and multi-class classifiers, respectively. They can be used to index handball videos to facilitate retrieval automatically. Finally, some open issues, challenges in applying deep learning methods in such a dynamic sports environment, and direction for future development will be discussed.},
DOI = {10.3390/jimaging9040080}
}

@article{giveki_human_2024,
	title = {Human action recognition using an optical flow-gated recurrent neural network},
	volume = {13},
	issn = {2192-662X},
	url = {https://doi.org/10.1007/s13735-024-00338-4},
	doi = {10.1007/s13735-024-00338-4},
	abstract = {Recognizing various human actions in videos is considered a highly complicated problem, which has many potential applications in solving real-world problems such as human behavior analysis, artificial intelligence, video surveillance, and smart manufacturing. Therefore, designing novel approaches for automatically understanding video data is highly demanded. Towards this goal, different algorithms have been investigated, concentrating on extracting the spatial information and the temporal dependencies. However, motion feature extraction is engineered and isolated from the learning operations. In this paper, to comprehend motion features along with the spatial information and the time dependencies, an innovative attempt is made by designing a new Gated Recurrent Unit (GRU) network. Moreover, a novel deep neural network is presented using the proposed GRU to recognize human actions. Evaluations on popular datasets (YouTube2011, UCF50, UCF101, and HMDB51) not only convey the superiority of the proposed GRU in action recognition using an end-to-end learning model but also emphasize on the generalizability of the proposed method. Additionally, to show the applicability and functionality of the proposed model in solving real-world problems, an engine block assembly dataset was collected and the performance of the proposed method was measured on this dataset. Finally, the robustness of the proposed method against various kinds of noise was tested. The obtained results demonstrate the high performance of the proposed method and its robustness against noise.},
	language = {en},
	number = {3},
	urldate = {2024-11-29},
	journal = {International Journal of Multimedia Information Retrieval},
	author = {Giveki, Davar},
	month = jul,
	year = {2024},
	keywords = {Action recognition, Artificial Intelligence, Convolutional neural networks, Gated recurrent unit, Motion feature, Spatial feature},
	pages = {29},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\URTQAJD3\\Giveki - 2024 - Human action recognition using an optical flow-gated recurrent neural network.pdf:application/pdf},
}

@ARTICLE{li_oarnet_2024,
  author={Li, Mingzhe and Duan, Yiping and Tao, Xiaoming and Chen, Changwen},
  journal={IEEE Transactions on Multimedia}, 
  title={OARNet: Object-Attribute-Relation Network for Predicting Soccer Events}, 
  year={2024},
  volume={26},
  number={},
  pages={9216-9227},
  keywords={Sports;Games;Feature extraction;Trajectory;Task analysis;Computational modeling;Visualization;Object-attribute-relation (OAR) model;adversarial event prediction;multimodal data;graph neural network},
  doi={10.1109/TMM.2024.3387724}}


@misc{bertasius_timesformer_2021,
	title = {Is {Space}-{Time} {Attention} {All} {You} {Need} for {Video} {Understanding}?},
	url = {http://arxiv.org/abs/2102.05095},
	doi = {10.48550/arXiv.2102.05095},
	abstract = {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
	month = jun,
	year = {2021},
	note = {arXiv:2102.05095},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\7NNJI98J\\Bertasius et al. - 2021 - Is Space-Time Attention All You Need for Video Understanding.pdf:application/pdf;Preprint PDF:C\:\\Users\\jofal\\Zotero\\storage\\JHBYEGVK\\Bertasius et al. - 2021 - Is Space-Time Attention All You Need for Video Understanding.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\RRHDKFWW\\2102.html:text/html},
}

@misc{arnab_vvit_2021,
      title={ViViT: A Video Vision Transformer}, 
      author={Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lucic and Cordelia Schmid},
      year={2021},
      eprint={2103.15691},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.15691}, 
}


@inproceedings{denize_comedian_2024,
	title = {{COMEDIAN}: {Self}-{Supervised} {Learning} and {Knowledge} {Distillation} for {Action} {Spotting} {Using} {Transformers}},
	shorttitle = {{COMEDIAN}},
	url = {https://ieeexplore.ieee.org/document/10495649/?arnumber=10495649},
	doi = {10.1109/WACVW60836.2024.00060},
	abstract = {We present COMEDIAN, a novel pipeline to initialize spatiotemporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence compared to non-pretrained models. Source code is available here: https:/github.com/juliendenize/eztorch.},
	urldate = {2024-11-28},
	booktitle = {2024 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} {Workshops} ({WACVW})},
	author = {Denize, Julien and Liashuha, Mykola and Rabarisoa, Jaonary and Orcesi, Astrid and Hérault, Romain},
	month = jan,
	year = {2024},
	note = {ISSN: 2690-621X},
	keywords = {Transformers, Spatiotemporal phenomena, Labeling, Costs, Pipelines, Self-supervised learning, Source coding},
	pages = {518--528},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\DDB4SMHH\\Denize et al. - 2024 - COMEDIAN Self-Supervised Learning and Knowledge Distillation for Action Spotting Using Transformers.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\jofal\\Zotero\\storage\\X9YYEDKK\\10495649.html:text/html},
}

@article{sarraf_optimal_2023,
	title = {Optimal {Topology} of {Vision} {Transformer} for {Real}-{Time} {Video} {Action} {Recognition} in an {End}-{To}-{End} {Cloud} {Solution}},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180293105&doi=10.3390%2fmake5040067&partnerID=40&md5=ca9c96de985aba38cb1c798b0826a9a5},
	doi = {10.3390/make5040067},
	abstract = {This study introduces an optimal topology of vision transformers for real-time video action recognition in a cloud-based solution. Although model performance is a key criterion for real-time video analysis use cases, inference latency plays a more crucial role in adopting such technology in real-world scenarios. Our objective is to reduce the inference latency of the solution while admissibly maintaining the vision transformer’s performance. Thus, we employed the optimal cloud components as the foundation of our machine learning pipeline and optimized the topology of vision transformers. We utilized UCF101, including more than one million action recognition video clips. The modeling pipeline consists of a preprocessing module to extract frames from video clips, training two-dimensional (2D) vision transformer models, and deep learning baselines. The pipeline also includes a postprocessing step to aggregate the frame-level predictions to generate the video-level predictions at inference. The results demonstrate that our optimal vision transformer model with an input dimension of 56 × 56 × 3 with eight attention heads produces an F1 score of 91.497\% for the testing set. The optimized vision transformer reduces the inference latency by 40.70\%, measured through a batch-processing approach, with a 55.63\% faster training time than the baseline. Lastly, we developed an enhanced skip-frame approach to improve the inference latency by finding an optimal ratio of frames for prediction at inference, where we could further reduce the inference latency by 57.15\%. This study reveals that the vision transformer model is highly optimizable for inference latency while maintaining the model performance. © 2023 by the authors.},
	number = {4},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Sarraf, S. and Kabia, M.},
	year = {2023},
	keywords = {action recognition, cloud solution, vision transformer},
	pages = {1320--1339},
}

@misc{lee_enhancing_mamba_s6_2024,
	title = {Enhancing {Temporal} {Action} {Localization}: {Advanced} {S6} {Modeling} with {Recurrent} {Mechanism}},
	shorttitle = {Enhancing {Temporal} {Action} {Localization}},
	url = {http://arxiv.org/abs/2407.13078},
	doi = {10.48550/arXiv.2407.13078},
	abstract = {Temporal Action Localization (TAL) is a critical task in video analysis, identifying precise start and end times of actions. Existing methods like CNNs, RNNs, GCNs, and Transformers have limitations in capturing long-range dependencies and temporal causality. To address these challenges, we propose a novel TAL architecture leveraging the Selective State Space Model (S6). Our approach integrates the Feature Aggregated Bi-S6 block, Dual Bi-S6 structure, and a recurrent mechanism to enhance temporal and channel-wise dependency modeling without increasing parameter complexity. Extensive experiments on benchmark datasets demonstrate state-of-the-art results with mAP scores of 74.2\% on THUMOS-14, 42.9\% on ActivityNet, 29.6\% on FineAction, and 45.8\% on HACS. Ablation studies validate our method's effectiveness, showing that the Dual structure in the Stem module and the recurrent mechanism outperform traditional approaches. Our findings demonstrate the potential of S6-based models in TAL tasks, paving the way for future research.},
	urldate = {2024-11-30},
	publisher = {arXiv},
	author = {Lee, Sangyoun and Jung, Juho and Oh, Changdae and Yun, Sunghee},
	month = jul,
	year = {2024},
	note = {arXiv:2407.13078},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\5YENLVBM\\Lee et al. - 2024 - Enhancing Temporal Action Localization Advanced S6 Modeling with Recurrent Mechanism.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\W2Z9SVZW\\2407.html:text/html},
}

@misc{tong_videomae_2022,
	title = {{VideoMAE}: {Masked} {Autoencoders} are {Data}-{Efficient} {Learners} for {Self}-{Supervised} {Video} {Pre}-{Training}},
	shorttitle = {{VideoMAE}},
	url = {http://arxiv.org/abs/2203.12602},
	doi = {10.48550/arXiv.2203.12602},
	abstract = {Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90\% to 95\%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4\% on Kinetics-400, 75.4\% on Something-Something V2, 91.3\% on UCF101, and 62.6\% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
	month = oct,
	year = {2022},
	note = {arXiv:2203.12602},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\REJMH4FL\\Tong et al. - 2022 - VideoMAE Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\4Z22ATUJ\\2203.html:text/html},
}

@misc{wang_videomae_2023,
	title = {{VideoMAE} {V2}: {Scaling} {Video} {Masked} {Autoencoders} with {Dual} {Masking}},
	shorttitle = {{VideoMAE} {V2}},
	url = {http://arxiv.org/abs/2303.16727},
	doi = {10.48550/arXiv.2303.16727},
	abstract = {Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-training on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0\% on K400 and 89.9\% on K600) and Something-Something (68.7\% on V1 and 77.0\% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner. The code and model is available at {\textbackslash}url\{https://github.com/OpenGVLab/VideoMAEv2\}.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Wang, Limin and Huang, Bingkun and Zhao, Zhiyu and Tong, Zhan and He, Yinan and Wang, Yi and Wang, Yali and Qiao, Yu},
	month = apr,
	year = {2023},
	note = {arXiv:2303.16727},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\S3SM42WL\\Wang et al. - 2023 - VideoMAE V2 Scaling Video Masked Autoencoders with Dual Masking.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\WS9S6IHZ\\2303.html:text/html},
}

@misc{li_videomamba_2024,
	title = {{VideoMamba}: {State} {Space} {Model} for {Efficient} {Video} {Understanding}},
	shorttitle = {{VideoMamba}},
	url = {http://arxiv.org/abs/2403.06977},
	doi = {10.48550/arXiv.2403.06977},
	abstract = {Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain. The proposed VideoMamba overcomes the limitations of existing 3D convolution neural networks and video transformers. Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding. Extensive evaluations reveal VideoMamba's four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts. Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding. All the code and models are available at https://github.com/OpenGVLab/VideoMamba.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Li, Kunchang and Li, Xinhao and Wang, Yi and He, Yinan and Wang, Yali and Wang, Limin and Qiao, Yu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.06977},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\TYJWDEUH\\Li et al. - 2024 - VideoMamba State Space Model for Efficient Video Understanding.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\L4K55X67\\2403.html:text/html},
}

@inproceedings{bose_soccerkdnet_2023,
	address = {Cham},
	title = {{SoccerKDNet}: {A} {Knowledge} {Distillation} {Framework} for {Action} {Recognition} in {Soccer} {Videos}},
	isbn = {978-3-031-45170-6},
	abstract = {Classifying player actions from soccer videos is a challenging problem, which has become increasingly important in sports analytics over the years. Most state-of-the-art methods employ highly complex offline networks, which makes it difficult to deploy such models in resource constrained scenarios. Here, in this paper we propose a novel end-to-end knowledge distillation based transfer learning network pre-trained on the Kinetics400 dataset and then perform extensive analysis on the learned framework by introducing a unique loss parameterization. We also introduce a new dataset named “SoccerDB1” containing 448 videos and consisting of 4 diverse classes each of players playing soccer. Furthermore, we introduce an unique loss parameter that help us linearly weigh the extent to which the predictions of each network are utilized. Finally, we also perform a thorough performance study using various changed hyperparameters. We also benchmark the first classification results on the new SoccerDB1 dataset obtaining 67.20\% validation accuracy. The dataset has been made publicly available at: https://bit.ly/soccerdb1.},
	booktitle = {Pattern {Recognition} and {Machine} {Intelligence}},
	publisher = {Springer Nature Switzerland},
	author = {Bose, Sarosij and Sarkar, Saikat and Chakrabarti, Amlan},
	editor = {Maji, Pradipta and Huang, Tingwen and Pal, Nikhil R. and Chaudhury, Santanu and De, Rajat K.},
	year = {2023},
	pages = {457--464},
}

@InProceedings{radhakrishnan_bi_lstm_2023,
author="Radhakrishnan, Rahulan
and AlZoubi, Alaa",
editor="de Sousa, A. Augusto
and Debattista, Kurt
and Paljic, Alexis
and Ziat, Mounia
and Hurter, Christophe
and Purchase, Helen
and Farinella, Giovanni Maria
and Radeva, Petia
and Bouatouch, Kadi",
title="Automatic Bi-LSTM Architecture Search Using Bayesian Optimisation for Vehicle Activity Recognition",
booktitle="Computer Vision, Imaging and Computer Graphics Theory and Applications",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="108--134",
abstract="This paper presents a novel method to find optimal Bidirectional Long-Short Term Memory Neural Network (Bi-LSTM) using Bayesian Optimisation method for vehicle trajectory classification. We extend our previous approach to be able to classify a larger number of vehicle trajectories collected from different sources in a single Bi-LSTM network. We also explored the use of deep learning visual explainability by highlighting the parts of the activity (or trajectory) contribute to the classification decision of the network. In particular, Qualitative Trajectory Calculus (QTC), spatio-temporal calculus, method is used to encode the relative movement between vehicles as a trajectory of QTC states. We then develop a Bi-LSTM network (called VNet) to classify QTC trajectories that represent vehicle pairwise activities. Existing Bi-LSTM networks for vehicle activity analysis are manually designed without considering the optimisation of the whole architecture nor its trainable hyperparameters. Therefore, we adapt Bayesian Optimisation method to search for an optimal Bi-LSTM architecture for classifying QTC trajectories of vehicle interaction. To test the validity of the proposed VNet, four datasets of 8237 trajectories of 9 unique vehicle activities in different traffic scenarios are used. We further compare our VNet model's performance with the state-of-the-art methods. The results on the combined dataset (accuracy of 98.21{\%}) showed that the proposed method generates light and most robust Bi-LSTM model. We also demonstrate that Activation Map is a promising approach for visualising the Bi-LSTM model decisions for vehicle activity recognition.",
isbn="978-3-031-45725-8"
}

@article{elaoud_skeleton-based_2020,
	title = {Skeleton-based comparison of throwing motion for handball players},
	volume = {11},
	issn = {1868-5145},
	url = {https://doi.org/10.1007/s12652-019-01301-6},
	doi = {10.1007/s12652-019-01301-6},
	abstract = {The main goal of this work is to design an automated solution based on RGB-D data for quantitative analysis, perceptible evaluation and comparison of handball player’s performance. To that end, we introduced a new RGB-D dataset that can be used for an objective comparison and evaluation of handball player’s performance during throws. We filmed 62 handball players (44 beginners and 18 experts), who performed the same type of action, using a Kinect V2 sensor that provides RGB data, depth data and skeletons. Moreover, using skeleton data simulating 3D joint connections, we examined the main angles responsible for throwing performance in order to analyze individual skills of handball players (beginners against model and experts) relatively to throw actions. The comparison was performed statically (using only one frame) as well as dynamically during the entire throwing action. In particular, given the temporal sequence of 25 joints of each handball player, we adopted the dynamic time warping technique to compare the throwing motion between two athletes. The obtained results were found to be promising. Thus, the suggested markless solution would help handball coaches to optimize beginners’ movements during throwing actions.},
	language = {en},
	number = {1},
	urldate = {2024-11-28},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Elaoud, Amani and Barhoumi, Walid and Zagrouba, Ezzeddine and Agrebi, Brahim},
	month = jan,
	year = {2020},
	keywords = {Artificial Intelligence, Dynamic time warping, Handball, Kinect V2, Performance evaluation, Skeleton},
	pages = {419--431},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\DUFH9FVI\\Elaoud et al. - 2020 - Skeleton-based comparison of throwing motion for handball players.pdf:application/pdf},
}

@article{wang_skeleton_two-stream_2023,
	title = {Two-stream spatiotemporal networks for skeleton action recognition},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164683927&doi=10.1049%2fipr2.12868&partnerID=40&md5=eba7080c1a065ba6f6d2e7466b5b01fc},
	doi = {10.1049/ipr2.12868},
	abstract = {Skeleton-based neural networks have been considered a focus for human action recognition (HAR). It is noteworthy that the existing skeleton-based methods are not capable of combining the spatial and temporal features reasonably to derive more effective high-level representations, and it continues to be a challenging task of learning and representing the skeleton action discriminatively. In this study, a novel two-stream spatiotemporal network (TSTN) is proposed, which is capable of processing the spatial and temporal features respectively and collectively to achieve a better representation and understanding of human action. The temporal branch stacks three gate recurrent unit (GRU) blocks in a new architecture to encode the temporal correlations from different aspects of human action, achieving high-level temporal semantic feature expressions. The spatial branch encodes the spatial features with multi-stacked graph convolutional network (GCN) blocks. Self-attention mechanisms incorporated with the graph structure of the skeleton are explored to add weight influence and structural hints to further enhance the performance. The experimental results verify the effectiveness and superiority of the proposed model in skeleton action recognition; the model reaches state-of-the-art on specific datasets. © 2023 The Authors. IET Image Processing published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
	number = {11},
	journal = {IET Image Processing},
	author = {Wang, L. and Zhang, J. and Yang, S. and Gu, S.},
	year = {2023},
	keywords = {Action recognition, character recognition, Character recognition, computer vision, Computer vision, Encoding (symbols), graph theory, Graph theory, Human actions, Human-action recognition, image recognition, Image recognition, Musculoskeletal system, neural nets, Neural-networks, pattern recognition, Security systems, Semantics, Spatial features, Spatiotemporal networks, Temporal correlations, Temporal features, Two-stream, video surveillance, Video surveillance},
	pages = {3358--3370},
}

@misc{reilly__skeleton_just_pi_2023,
	title = {Just {Add} $\pi$! {Pose} {Induced} {Video} {Transformers} for {Understanding} {Activities} of {Daily} {Living}},
	url = {http://arxiv.org/abs/2311.18840},
	doi = {10.48550/arXiv.2311.18840},
	abstract = {Video transformers have become the de facto standard for human action recognition, yet their exclusive reliance on the RGB modality still limits their adoption in certain domains. One such domain is Activities of Daily Living (ADL), where RGB alone is not sufficient to distinguish between visually similar actions, or actions observed from multiple viewpoints. To facilitate the adoption of video transformers for ADL, we hypothesize that the augmentation of RGB with human pose information, known for its sensitivity to fine-grained motion and multiple viewpoints, is essential. Consequently, we introduce the first Pose Induced Video Transformer: PI-ViT (or \${\textbackslash}pi\$-ViT), a novel approach that augments the RGB representations learned by video transformers with 2D and 3D pose information. The key elements of \${\textbackslash}pi\$-ViT are two plug-in modules, 2D Skeleton Induction Module and 3D Skeleton Induction Module, that are responsible for inducing 2D and 3D pose information into the RGB representations. These modules operate by performing pose-aware auxiliary tasks, a design choice that allows \${\textbackslash}pi\$-ViT to discard the modules during inference. Notably, \${\textbackslash}pi\$-ViT achieves the state-of-the-art performance on three prominent ADL datasets, encompassing both real-world and large-scale RGB-D datasets, without requiring poses or additional computational overhead at inference.},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Reilly, Dominick and Das, Srijan},
	month = nov,
	year = {2023},
	note = {arXiv:2311.18840 
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\A7H5M8J3\\Reilly and Das - 2023 - Just Add \$π\$! Pose Induced Video Transformers for Understanding Activities of Daily Living.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\ZM76LQ5P\\2311.html:text/html},
}

@misc{xarles_t-deed_2024,
	title = {T-{DEED}: {Temporal}-{Discriminability} {Enhancer} {Encoder}-{Decoder} for {Precise} {Event} {Spotting} in {Sports} {Videos}},
	shorttitle = {T-{DEED}},
	url = {http://arxiv.org/abs/2404.05392},
	doi = {10.48550/arXiv.2404.05392},
	abstract = {In this paper, we introduce T-DEED, a Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in sports videos. T-DEED addresses multiple challenges in the task, including the need for discriminability among frame representations, high output temporal resolution to maintain prediction precision, and the necessity to capture information at different temporal scales to handle events with varying dynamics. It tackles these challenges through its specifically designed architecture, featuring an encoder-decoder for leveraging multiple temporal scales and achieving high output temporal resolution, along with temporal modules designed to increase token discriminability. Leveraging these characteristics, T-DEED achieves SOTA performance on the FigureSkating and FineDiving datasets. Code is available at https://github.com/arturxe2/T-DEED.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Xarles, Artur and Escalera, Sergio and Moeslund, Thomas B. and Clapés, Albert},
	month = apr,
	year = {2024},
	note = {arXiv:2404.05392},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\jofal\\Zotero\\storage\\8JYHGKHY\\Xarles et al. - 2024 - T-DEED Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in Sports Vide.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\69JR5WPJ\\2404.html:text/html},
}
@inproceedings{gerats_individual_same_task_2021,
	title = {Individual {Action} and {Group} {Activity} {Recognition} in {Soccer} {Videos} from a {Static} {Panoramic} {Camera}},
	url = {https://research.utwente.nl/en/publications/individual-action-and-group-activity-recognition-in-soccer-videos},
	doi = {10.5220/0010303505940601},
	language = {English},
	urldate = {2024-11-28},
	booktitle = {{ICPRAM} 2021 - {Proceedings} of the 10th {International} {Conference} on {Pattern} {Recognition} {Applications} and {Methods}},
	publisher = {SCITEPRESS},
	author = {Gerats, Beerend and Bouma, Henri and Uijens, Wouter and Englebienne, Gwenn and Spreeuwers, Luuk},
	year = {2021},
	pages = {594--601},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\CUW3ZLQB\\Gerats et al. - 2021 - Individual Action and Group Activity Recognition in Soccer Videos from a Static Panoramic Camera.pdf:application/pdf},
}


@misc{cioppa_soccernet_2024,
	title = {{SoccerNet} 2024 {Challenges} {Results}},
	url = {http://arxiv.org/abs/2409.10587},
	doi = {10.48550/arXiv.2409.10587},
	abstract = {The SoccerNet 2024 challenges represent the fourth annual video understanding challenges organized by the SoccerNet team. These challenges aim to advance research across multiple themes in football, including broadcast video understanding, field understanding, and player understanding. This year, the challenges encompass four vision-based tasks. (1) Ball Action Spotting, focusing on precisely localizing when and which soccer actions related to the ball occur, (2) Dense Video Captioning, focusing on describing the broadcast with natural language and anchored timestamps, (3) Multi-View Foul Recognition, a novel task focusing on analyzing multiple viewpoints of a potential foul incident to classify whether a foul occurred and assess its severity, (4) Game State Reconstruction, another novel task focusing on reconstructing the game state from broadcast videos onto a 2D top-view map of the field. Detailed information about the tasks, challenges, and leaderboards can be found at https://www.soccer-net.org, with baselines and development kits available at https://github.com/SoccerNet.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Cioppa, Anthony and Giancola, Silvio and Somers, Vladimir and Joos, Victor and Magera, Floriane and Held, Jan and Ghasemzadeh, Seyed Abolfazl and Zhou, Xin and Seweryn, Karolina and Kowalczyk, Mateusz and Mróz, Zuzanna and Łukasik, Szymon and Hałoń, Michał and Mkhallati, Hassan and Deliège, Adrien and Hinojosa, Carlos and Sanchez, Karen and Mansourian, Amir M. and Miralles, Pierre and Barnich, Olivier and Vleeschouwer, Christophe De and Alahi, Alexandre and Ghanem, Bernard and Droogenbroeck, Marc Van and Gorski, Adam and Clapés, Albert and Boiarov, Andrei and Afanasiev, Anton and Xarles, Artur and Scott, Atom and Lim, ByoungKwon and Yeung, Calvin and Gonzalez, Cristian and Rüfenacht, Dominic and Pacilio, Enzo and Deuser, Fabian and Altawijri, Faisal Sami and Cachón, Francisco and Kim, HanKyul and Wang, Haobo and Choe, Hyeonmin and Kim, Hyunwoo J. and Kim, Il-Min and Kang, Jae-Mo and Tursunboev, Jamshid and Yang, Jian and Hong, Jihwan and Lee, Jimin and Zhang, Jing and Lee, Junseok and Zhang, Kexin and Habel, Konrad and Jiao, Licheng and Li, Linyi and Gutiérrez-Pérez, Marc and Ortega, Marcelo and Li, Menglong and Lopatto, Milosz and Kasatkin, Nikita and Nemtsev, Nikolay and Oswald, Norbert and Udin, Oleg and Kononov, Pavel and Geng, Pei and Alotaibi, Saad Ghazai and Kim, Sehyung and Ulasen, Sergei and Escalera, Sergio and Zhang, Shanshan and Yang, Shuyuan and Moon, Sunghwan and Moeslund, Thomas B. and Shandyba, Vasyl and Golovkin, Vladimir and Dai, Wei and Chung, WonTaek and Liu, Xinyu and Zhu, Yongqiang and Kim, Youngseo and Li, Yuan and Yang, Yuting and Xiao, Yuxuan and Cheng, Zehua and Li, Zhihao},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10587},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\YTB5APZY\\Cioppa et al. - 2024 - SoccerNet 2024 Challenges Results.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\NKJY4LDL\\2409.html:text/html},
}

@misc{gu_mamba_2024,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = may,
	year = {2024},
	note = {arXiv:2312.00752},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\VFDD9WJY\\Gu and Dao - 2024 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\IDP9LDAU\\2312.html:text/html},
}

@misc{deliege_soccernet-v2_dataset_2021,
	title = {{SoccerNet}-v2: {A} {Dataset} and {Benchmarks} for {Holistic} {Understanding} of {Broadcast} {Soccer} {Videos}},
	shorttitle = {{SoccerNet}-v2},
	url = {http://arxiv.org/abs/2011.13367},
	doi = {10.48550/arXiv.2011.13367},
	abstract = {Understanding broadcast videos is a challenging task in computer vision, as it requires generic reasoning capabilities to appreciate the content offered by the video editing. In this work, we propose SoccerNet-v2, a novel large-scale corpus of manual annotations for the SoccerNet video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production. Specifically, we release around 300k annotations within SoccerNet's 500 untrimmed broadcast soccer videos. We extend current tasks in the realm of soccer to include action spotting, camera shot segmentation with boundary detection, and we define a novel replay grounding task. For each task, we provide and discuss benchmark results, reproducible with our open-source adapted implementations of the most relevant works in the field. SoccerNet-v2 is presented to the broader research community to help push computer vision closer to automatic solutions for more general video understanding and production purposes.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Deliège, Adrien and Cioppa, Anthony and Giancola, Silvio and Seikavandi, Meisam J. and Dueholm, Jacob V. and Nasrollahi, Kamal and Ghanem, Bernard and Moeslund, Thomas B. and Droogenbroeck, Marc Van},
	month = apr,
	year = {2021},
	note = {arXiv:2011.13367},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\jofal\\Zotero\\storage\\4BZ5YSIU\\Deliège et al. - 2021 - SoccerNet-v2 A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos.pdf:application/pdf;Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\PRSVGZHL\\2011.html:text/html},
}

@article{chen_children_2023,
	title = {Children's {Football} {Action} {Recognition} based on {LSTM} and a {V}-{DBN}},
	volume = {12},
	url = {https://www.dbpia.co.kr},
	doi = {10.5573/IEIESPC.2023.12.4.312},
	abstract = {In order to improve teaching children how to play football, combining the Vector of Locally Aggregated Descriptors (VLAD) model and a deep belief network (DBN) into a V-DBN is proposed based on 3D bone recognition that recognizes football actions. We use the contrast method to reduce the dimensionality of action features, and we complete the action recognition through analysis of key parameters. After experimental testing with the MSRAction3D data set, Grassmann manifold and graph-based action classification and recognition reach accuracies of 79.2\% and 93.4\%, respectively, after 100 iterations of training, but the V-DBN reaches 98.6\%. In the UTKinect-Action database test, the average recognition rates of Grassmann manifold and graph-based action classification and recognition are 88.38\% and 91.31\% accurate, respectively, while the VLAD is 93.96\% accurate, showing the best overall performance. However, the effect in single-action recognition is only average. Using the LSTM optimization model on results from infant football action recognition, the average accuracy rate of LSTM+V-DBN is 0.981 compared to the V-DBN at 0.892. Clearly, the optimized LSTM+V-DBN model performs better in toddler action recognition. This research provides important reference value to the application of human action recognition technology in children’s football education.},
	language = {ko},
	number = {4},
	urldate = {2024-11-28},
	journal = {IEIE Transactions on Smart Processing \& Computing},
	author = {Chen, Zhaosheng and Chen, Na},
	month = aug,
	year = {2023},
	pages = {312--322},
	file = {Snapshot:C\:\\Users\\jofal\\Zotero\\storage\\S9G7WXZP\\articleDetail.html:text/html},
}

@misc{dataset:kinetics,
      title={The Kinetics Human Action Video Dataset}, 
      author={Will Kay and Joao Carreira and Karen Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and Trevor Back and Paul Natsev and Mustafa Suleyman and Andrew Zisserman},
      year={2017},
      eprint={1705.06950},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1705.06950}, 
}

@article{dataset:thumos,
   title={The THUMOS challenge on action recognition for videos “in the wild”},
   volume={155},
   ISSN={1077-3142},
   url={http://dx.doi.org/10.1016/j.cviu.2016.10.018},
   DOI={10.1016/j.cviu.2016.10.018},
   journal={Computer Vision and Image Understanding},
   publisher={Elsevier BV},
   author={Idrees, Haroon and Zamir, Amir R. and Jiang, Yu-Gang and Gorban, Alex and Laptev, Ivan and Sukthankar, Rahul and Shah, Mubarak},
   year={2017},
   month=feb, pages={1–23} }


@article{arnold_deep_learning_2011,
	title = {An {Introduction} to {Deep} {Learning}},
	abstract = {The deep learning paradigm tackles problems on which shallow architectures (e.g. SVM) are aﬀected by the curse of dimensionality. As part of a two-stage learning scheme involving multiple layers of nonlinear processing a set of statistically robust features is automatically extracted from the data. The present tutorial introducing the ESANN deep learning special session details the state-of-the-art models and summarizes the current understanding of this learning approach which is a reference for many diﬃcult classiﬁcation tasks.},
	language = {en},
	journal = {Computational Intelligence},
	author = {Arnold, Ludovic and Rebecchi, Sébastien and Chevallier, Sylvain and Paugam-Moisy, Hélène},
	year = {2011},
	file = {PDF:C\:\\Users\\jofal\\Zotero\\storage\\IURGMCZV\\Arnold et al. - 2011 - An Introduction to Deep Learning.pdf:application/pdf},
}

@article{lecun_deep_learning_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2025-04-29},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
	file = {Full Text PDF:C\:\\Users\\jofal\\Zotero\\storage\\F9JTQEX2\\LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@misc{sanderson_deep_learning_2024,
	author = {Sanderson, Grant},
	month = {8},
	title = {{Neural Networks}},
	year = {2024},
	url = {https://www.3blue1brown.com/topics/neural-networks},
}