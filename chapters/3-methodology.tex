\chapter{Methodology} 
\label{chap:methodology}
This chapter describes the experimental setup used to evaluate the \acrfull{s6} model for temporal event spotting in football video. The section details computing infrastructure, data preprocessing and feature extraction pipeline, model architectures, training protocols, and evaluation metrics. All aligned with the research questions (\autoref{sec:research_questions}).


\section{Tools and Resources}
\label{sec:tools_and_resources}

\subsection{IDUN}
\label{ssec:idun}
Video processing pipelines demand substantial computational resources. IDUN is a faculty-operated system at NTNU equipped with 234 NVIDIA \acrfull{gpu} accelerators (as of the 24th of April, 2025)\footnote{\url{https://www.hpc.ntnu.no/idun/}}. Although IDUN is not NTNU's central supercomputer, its nodes, featuring Tesla H100 and A100 \acrshort{gpu}s, with up to 80 GB of RAM, provided the necessary throughput for feature extraction and model training. Full-match videos were partitioned into fixed-duration clips and distributed workloads across multiple \acrshort{gpu}s via data-parallel training to mitigate occasional queuing delays and VRAM limitations.

Due to the high memory requirements of the \acrfull{tdeed} and software requirements of the \acrfull{s6} framework, all experiments run on IDUN nodes. Attempts to train on \acrshort{gpu}s with 16 GB of memory trigger out-of-memory errors with \acrshort{tdeed}. Moreover, MAMBA \acrshort{s6} requires specific CUDA versions and matching libraries, which older driver stacks do not support. Model development and evaluation are restricted to nodes that meet these hardware and software requirements to ensure stable training and reproducible results. 



\subsection{\acrfull{wandb}}
\label{ssec:wandb}

The acrlong{wandb} platform streamlined the management and reproducibility of the training experiments. Hyperparameters are logged automatically using \acrshort{wandb}'s Python client, as are training and validation metrics, model checkpoints, and \acrshort{gpu} utilization statistics. A dashboard enables real-time monitoring of experiments, as well as direct and live comparison of runs. \acrshort{wandb}'s automated hyperparameter sweep feature was used to conduct hyperparameter searches. Coordinated sweeps log performance metrics and visualized the results. \acrshort{wandb} enhances the transparency, comparability, and reproducibility of workflows. 

\subsection{Bayesian Hyperparameter Search}

Bayesian optimization uses probability to tune hyperparameters. It balances exploring new hyperparameter values with exploiting known good ones. The method uses a Gaussian Process as a surrogate model to predict the \acrlong{map} performance metric.


The optimization process follows these steps:
\begin{itemize}
    \item \textbf{Surrogate Model}: A probabilistic model estimates the objective function based on previous evaluations.
    \item \textbf{Acquisition Function}: Determines the next hyperparameter set to evaluate by optimizing a function such as Expected Improvement (EI) or Upper Confidence Bound (UCB).
    \item \textbf{Evaluation and Update}: The chosen hyperparameters are tested, and the surrogate model is updated with new information.
    \item \textbf{Iteration}: The process repeats until convergence or a stopping criterion is met.
\end{itemize}

Bayesian optimization is beneficial for deep learning models because testing each hyperparameter set requires significant computing power. It utilizes probability to determine the optimal hyperparameters, thereby reducing the number of tests needed.


\subsection{Comparing Correlation and Importance}

While correlation identifies linear dependencies, importance seeks for complex interactions among hyperparameters. This distinction enables researchers to make informed decisions when designing hyperparameter search spaces. Bayesian optimization enhances this process by selecting hyperparameters based on probabilistic predictions. The predictions lead to faster convergence.

\subsection{Anaconda and pip}
\label{ssec:conda_pip}
Conda manages the Python environments. However, all packages, including CUDA-enabled PyTorch builds, were installed via pip to follow recent PyTorch recommendations. Those recommendations favor pip over conda for \acrshort{gpu} support.

\section{Data Preprocessing \& Feature Extraction}
\label{sec:preprocessing}

Experiments are conducted on the SoccerNet-V2 dataset\cite{deliege_soccernet-v2_dataset_2021}. Videos are split into non-overlapping clips $V_k=\{x_t\}_{t=1}^{T_c}$ of fixed duration $T_c=60\!\times\!25$ frames (60\,s at 25\,fps). A 70-30 split and an 80-20 split were used for training and validation. The 80-20 split matches the official publication. 70-30 matches a PyTorch recommendation in the docs \footnote{\url{https://docs.pytorch.org/docs/stable/data.html}. Retrieved the 13th of May}.


\lstinputlisting[
    caption={Script to change the label-file to the THUMOS-14 format.},
    label=lst:jsonfile,
    language=Python
]{listings/sn_to_json.py}

\unsure{the code uses random.random to create a train/val split. To reproduce, should I upload the used data for reproducibility?}

The duration key is unnecessary in the case of temporal action localization, as shown in the \autoref{lst:uselessduration}, which sets a default value if the duration is not explicitly set. The video clip at the end of each half, $V_{N}$, has a $T_c$ that refers to the length of the clip. \acrfull{vms} handles different-length inputs. 

\lstinputlisting[
    caption={Part of \acrshort{vms} code used to determine the duration to be pointless. },
    label=lst:uselessduration,
    language=Python
]{listings/useless_duration.py}

Each clip $V_k$ is fed through the pre‐trained VideoMAE-V2 encoder (\autoref{ssec:videomae_v2}) to extract a feature embedding
\[
z_k = \mathrm{VideoMAE\text{-}V2}(V_k)\;\in\;\mathbb{R}^D.
\]
These $D$‐dimensional vectors serve as inputs to the downstream temporal MAMBA model. The vectors are saved on IDUN as \textit{*.pt} files. Storing the vectors offline eliminates redundant computations, reduces storage requirements, and significantly lowers training times. 

This simplified protocol was adopted without established guidelines for optimally splitting football footage. \info{In the literature study I did, I found nothing about this} Future research could explore overlapping windows, adaptive clip boundaries, or dynamic segmentation schemes to mitigate boundary artifacts and potentially improve temporal event localization.

Despite its demonstrated effectiveness in learning meaningful spatiotemporal representations, \acrfull{vmae} incurs substantial computational overhead that must be managed in large-scale experiments. The dual-stage masking strategy yields robust feature embeddings in which up to 90 \% of video tokens are hidden during training. However, it also increases the number of model parameters and the complexity of each forward pass. In practice, processing a single 60s clip at 25 fps requires on the order of tens of gigaflops\cite{wang_videomae_2023}. All \acrshort{vmae} embeddings are extracted and stored offline to mitigate these resource requirements. The heavy compute footprint of \acrshort{vmae} motivates the exploration of more lightweight architectures or more aggressive token reduction techniques in future work.

\acrshort{vmae} has been pre-trained on the large-scale Kinetics dataset. Consequently, its learned representations capture broad spatiotemporal patterns common to various athletic activities but are not explicitly tailored to football. While this generic pretraining supports robust feature extraction across multiple sports domains, specialized fine-tuning on football video may improve temporal event localization performance. Future work should explore domain-specific adaptation using dedicated football datasets to bridge the semantic gap between Kinetics pretraining and match-level footage. 

InternVideo is the foundation for the comparative experiment, which evaluates 1,408-dimensional feature vectors extracted by a Kinetics-pretrained model against 3,302-dimensional embeddings generated. Both architectures originate from the same research group that developed \acrshort{vmae}. InternVideo is a bigger, still open-source framework.


\section{Datasets}
\label{sec:method_datasets}

\subsection{THUMOS-14}
\label{ssec:method_thumos14}
The THUMOS-14 dataset \cite{dataset:thumos} is a widely used benchmark for temporal action localization. According to \hyperlink{https://paperswithcode.com/dataset/thumos14-1}{Papers With Code}, it is the most popular in the temporal action localization category as of the 29th of May 2025. It consists of over 13,000 temporally annotated action instances distributed across 20 distinct action classes. Its primary focus is on sports activities. The dataset provides untrimmed videos. It is suitable for evaluating models on their ability to detect and localize actions within longer video sequences. 

Key characteristics of THUMOS-14 include:
\begin{itemize}
    \item \textbf{Action Classes}: 20 diverse action categories.
    \item \textbf{Video Content}: Primarily sports-related, sourced from YouTube.
    \item \textbf{Annotations}: Provides temporal annotations (start and end times) for action instances in the validation and test sets.
    \item \textbf{Dataset Split}: Includes training, validation, and testing sets. The validation set contains 1010 videos, and the test set contains 1574 videos. Among these, there are 432 videos with temporal annotations, of which 220 are validation videos and 212 are labeled test videos. 
    \item \textbf{Evaluation}: Performance is typically measured using mean Average Precision (mAP) at various Intersection over Union (IoU) thresholds.
\end{itemize}

In this work, THUMOS-14 is relevant as it serves as a benchmark for many temporal action localization models, including \acrshort{vms} and RDFA-S6. The \acrshort{vms} implementation used in this study provides preprocessing scripts and configurations tailored for THUMOS-14.

\subsection{SoccerNet-V2}

The SoccerNet-V2 dataset \cite{deliege_soccernet-v2_dataset_2021} comprises video recordings of 7 professional football matches, along with fine-grained annotations for 12 action classes. It provides:
\begin{itemize}
    \item 7 full matches (90 min each) at 25 fps.
    \item Time-stamped action labels for spotting tasks.
    \item Team annotations for each event.
    \item Publicly accessible via HuggingFace\footnote{\url{https://huggingface.co/datasets/SoccerNet/SN-BAS-2025}}.
    \item The twelve football event classes:
        \begin{center}
            \begin{tabular}{llll}
                Pass & Drive & Header & High Pass \\
                Out & Cross & Throw In & Shot \\
                Ball Player Block & Player Successful Tackle & Free Kick & Goal
            \end{tabular}
        \end{center}
    \item An action approximately every 3 seconds. 
\end{itemize}

The data is locked behind a non-disclosure agreement (NDA). There are nine annotated games, of which seven are available, and the last two are related to the challenge. The nine games are from the second tier of English football, the Championship, and were played in October 2019. The games in question are: 

\begin{itemize}
    \item 2019-10-01 - Leeds United - West Bromwich
    \item 2019-10-01 - Hull City - Sheffield Wednesday
    \item 2019-10-01 - Brentford - Bristol City
    \item 2019-10-01 - Blackburn Rovers - Nottingham Forest
    \item 2019-10-01 - Middlesbrough - Preston North End
    \item 2019-10-01 - Stoke City - Huddersfield Town
    \item 2019-10-01 - Reading - Fulham
    \item 2019-10-02 - Cardiff City - Queens Park Rangers
    \item 2019-10-01 - Wigan Athletic - Birmingham City
\end{itemize}

The authors claim that the first four games are training games, and the fifth is a validation game. The sixth and seventh games are test games used for testing purposes, while the last two games do not have public annotations. They are the challenge games, as mentioned above. All videos are in the *.mkv file format; one can also download features for older videos. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/actions_per_class.png}
    \caption{Action distribution for different classes in the SoccerNet dataset for action ball spotting.}
    \label{fig:soccernet_dist}
\end{figure}

\cref{sec:preprocessing} provides a more in-depth explanation of how the SoccerNet labels are created, similar to the THUMOS labels, to align with the \acrshort{vms} model. 

The SoccerNet dataset contains 500 videos with temporal annotations, related to an earlier challenge. There are 17 action classes.
\begin{itemize}
    \item Penalty
    \item Kick-off
    \item Goal
    \item Substitution
    \item Offside
    \item Shots on target
    \item Shots off target
    \item Clearance
    \item Ball out of play
    \item Throw-in
    \item Foul
    \item Indirect free-kick
    \item Direct free-kick
    \item Corner
    \item Yellow card
    \item Red card
    \item Yellow->red card
\end{itemize}

The actions are associated with a single timestamp, and their action-density is lower compared the the new dataset.

\section{Model Architectures}
\label{sec:model_architectures}

\subsection{VideoMAE-V2}
\label{ssec:videomae_v2}

\subsubsection{Masking}

The input video frames are first tokenized into spatio-temporal patches in an encoder-decoder masking setup. A binary mask tensor \(M\in\{0,1\}^{T\times H\times W}\) with masking ratio \(r\) is generated, where
\[
\sum_{t,h,w} M_{t,h,w} = r\,T\,H\,W.
\]
The visible patches \(x_\text{vis}\) are obtained by
\[
x_\text{vis} = x \,\odot\,(1 - M)\,,
\]
and fed into a lightweight transformer encoder \(E\). The decoder \(D\) takes the encoder output, plus positional embeddings and mask tokens, and attempts to reconstruct the original video:
\[
\hat{x} = D\bigl(E(x_\text{vis}),\,\text{mask\_tokens}\bigr).
\]
The training process minimizes the reconstruction loss only over masked positions:
\[
\mathcal{L} \;=\; \bigl\lVert\,M \,\odot\,(x - \hat{x})\bigr\rVert_2^2.
\]


\acrfull{vmae} by \textcite{wang_videomae_2023} is a self‑supervised masked autoencoder designed to preprocess and compress large‑scale video data before downstream experiments. Building on the original VideoMAE\cite{tong_videomae_2022}, it introduces a dual‑stage masking schedule: 

\begin{itemize}
    \item \emph{Sparse spatiotemporal masking} in early epochs to encourage global context learning over long clips,
    \item \emph{Dense tube masking} in later epochs to refine local motion representations.
\end{itemize}

VideoMAE has three components: embedder, encoder, and decoder. VideoMAE uses cube embedding \(\Phi_{emb}\) to transform frames into sequences of tokens. It designs a tube masking strategy with a ratio of \(\rho \simeq 90\%\). The visible tokens are encoded by a vanilla \acrshort{vit} backbone. A decoder reconstructs the masked patches using another \acrshort{vit}\cite{wang_videomae_2023}. 
\improvement{There is some juicy math in the paper which can help the thesis seem more professional}

By dynamically varying the masking granularity, VideoMAE‑V2 achieves:
\begin{itemize}
    \item 2× faster convergence compared to VideoMAE,
    \item linear scaling to billion‑parameter models,
    \item robust transferability across action recognition, temporal localization, and segmentation tasks\cite{wang_videomae_2023}.
\end{itemize}

Pre-extraction of \acrshort{vmae} embeddings significantly reduces storage and computational demands. Saving compact feature vectors instead of raw video sequences lowers I/O overhead and accelerates data loading. This approach decouples frame encoding from model training, enabling more rapid and iterative experimentation. As a result, overall training time decreases, and resource utilization improves.

\subsection{\acrfull{s6}}
\label{ssec:s6}

\acrfull{ssm} offers a continuous-time representation of sequential data via a latent state \(h(t)\in\mathbb{R}^N\):
\begin{align}
    \frac{\mathrm{d}h(t)}{\mathrm{d}t} &= A\,h(t) + B\,x(t),  \label{eq:ssm_continuous1}\\
    y(t) &= C\,h(t),                                    \label{eq:ssm_continuous2}
\end{align}
where \(x(t)\in\mathbb{R}^d\) is the input, \(y(t)\in\mathbb{R}^m\) the output, and \(A\in\mathbb{R}^{N\times N}, B\in\mathbb{R}^{N\times d}, C\in\mathbb{R}^{m\times N}\) are learned parameters.  Discretizing these equations with step size \(\Delta\) yields
\begin{align}
    \bar A &= \exp(\Delta\,A), 
    & 
    \bar B &= \bigl(\exp(\Delta\,A)-I\bigr)A^{-1}B,\\
    h_k &= \bar A\,h_{k-1} + \bar B\,x_k, 
    &
    y_k &= C\,h_k,
    \quad k=1,2,\dots
\end{align}

VideoMamba \cite{li_videomamba_2024} builds on this framework with a \emph{Selective Scan} (\acrshort{s6}) core. Its parameters \((\Delta, A,B,C)\) control:
\begin{itemize}
    \item \(\Delta\): temporal resolution of the recurrence,
    \item \(A\): continuous dynamics matrix,
    \item \(B,C\): input and output mappings.
\end{itemize}

By preserving a compact hidden state, Mamba approximates long-range dependencies akin to self-attention but with only \(\mathcal{O}(L)\) time/memory cost rather than \(\mathcal{O}(L^2)\) of the \acrfull{vit}.  


\subsubsection{Mamba Block}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/mamba_algorithm.png}
\end{figure}

Mamba\cite{gu_mamba_2024} creates a selection mechanism for compressing context into a smaller state. It introduces input-dependent parameters to allow the model to either keep or discard information. The approach enables linear scaling with respect to sequence length. The algorithm utilizes functions to update the \(B\) and \(C\) matrices, as described in the algorithm design by \textcite{gu_mamba_2024}. The Mamba blocks are hardware-aware; they use kernel fusion, parallel scan, and recomputation to address the limitations of Limited Time Invariance. SSM parameters are directly loaded into fast SRAM memory on \acrshort{gpu}s. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/h3_mlp_mamba.png}
    \caption{Architecture and the components of a Mamba block.  Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared to the MLP block, Mamba adds an SSM to the main branch \cite{gu_mamba_2024}[Figure 3].} 
    \label{fig:h3_mlp_mamba}
\end{figure}



\subsubsection{\acrfull{vms}}
The \acrfull{vms} is a \acrfull{sota} implementation in the Papers with Code benchmark. It integrates a \acrlong{s6} optimized for long-range temporal dependencies and offers robust performance across various video event detection tasks. \cref{fig:mamba_ssm} shows the Mamba adoption they use to extend its performance into the video domain. The \acrfull{dbm} starts with distinct linear layers to segregate forward- and backward-features. Then both features are passed through \acrshort{ssm} with bidirectional scanning. The features undergo two gating layers before they are concatenated into an output token sequence\cite{li_videomamba_2024}. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/mamba_ssm.png}
    \caption{Three \acrshort{ssm} blocks. (a) is the vanilla Mamba block\cite{gu_mamba_2024}. (c) is the proposed \acrshort{dbm} block by \textcite{li_videomamba_2024}.}
    \label{fig:mamba_ssm}
\end{figure}

\subsubsection{RDFA-S6}
The RDFA-S6 architecture extends the \acrshort{vms} core with recurrent mechanisms. Empirical evaluations reveal that RDFA-S6 achieves slightly higher \acrlong{map} compared to \acrshort{vms}. This indicates a slight improvement in performance in event localization benchmarks on THUMOS-14, ActivityNet, FineAction, and HACS. 

Although RDFA-S6 showed marginally improved accuracy, limited documentation hindered its adoption. Compared to the well-documented \acrlong{vms}, RDFA-S6 documentation lacked central elements, such as datasets. Consequently, \acrshort{vms} remained this study's primary model for empirical evaluation. 


\subsection{\acrfull{tdeed}}
\label{ssec:tdeed}

\acrfull{tdeed} is a deep learning architecture designed to address the challenge of \acrfull{pes} in sports videos. This model enhances token discriminability while utilizing multiple temporal scales, thereby ensuring high-resolution event localization. The architecture consists of three main components: a feature extractor, a temporally discriminant encoder-decoder, and prediction heads\cite{xarles_t-deed_2024}.

The feature extractor generates per-frame feature representations. It employs a RegNetY-based backbone with \acrfull{gsf} modules, integrating local temporal context while maintaining spatial information. Given an input frame sequence of shape \(\mathbb{R}^{L \times H \times W \times 3}\), the extracted feature representation is formulated as:

\[
z \in \mathbb{R}^{L/{k^j \times d} }
\]

The encoder-decoder architecture enables processing across multiple temporal scales, capturing events that require varying amounts of temporal context. The encoder enhances token discriminability using \acrfull{sgp} layers, which mitigate similarity issues commonly present in adjacent frames. The temporal dimension is downscaled through max-pooling, followed by an upsampling process in the decoder. Skip connections are integrated into the decoder using the \acrshort{sgp}-Mixer layer, aggregating features from different temporal scales while preserving fine-grained details.

The prediction head consists of a classification head and a displacement head. The classification head predicts event occurrences per frame using a softmax function, while the displacement head improves predictions by estimating the precise event frame within a given radius. This ensures robust event localization even when slight spatial or temporal differences exist.


\acrshort{tdeed} trains end-to-end using a multi-task loss. The loss comprises a weighted cross-entropy loss \(\mathcal{L}_c\) and a displacement loss \(\mathcal{L}_d\), defined as

\[
\mathcal{L} = \frac{1}{L}\sum_{l=1}^{L}\Big(\mathcal{CE}_{w}(y_l^{c},\hat{y}_l^c) + \operatorname{MSE}(y_l^{d},\hat{y}_l^d)\Big).
\]

Here, \(y_l^{c}\) is the one-hot encoding of the event in frame \(l\). The probability distribution at frame \(l\) is denoted by \(\hat{y}_l^c\). Similarly, \(y_l^{d}\) and \(\hat{y}_l^d\) represent the ground truth and predicted displacements, respectively.

The experiments use a T-DEED implementation adapted for the SoccerNet challenge. Videos are turned into pictures of each frame and stored offline, following the original method. Event spotting performance is evaluated using \acrlong{map}.


\section{Training Details} \todo
% optimizer, learning rates, augmentation, early‐stopping, etc.

% In this study, we evaluated both \acrshort{vms} and RDFA-S6 architectures, striking a balance between performance and ease of use. RDFA-S6 achieved slightly higher event localization accuracy on benchmarks, yet its documentation and setup complexity introduced reproducibility challenges. By contrast, the \acrshort{vms} implementation provided comprehensive THUMOS dataset annotations, robust download links, and a well-structured run script. These features facilitated an easier integration into the experimental pipeline. Consequently, given its superior usability and reliable performance, \acrshort{vms} was chosen as the primary model.

% All training hyperparameters and architectural settings are specified in the experiments' external configuration files, ensuring reproducibility and transparency. An early stopping mechanism based on validation loss halts training when performance improvements plateau and/or decrease. While no explicit data augmentation pipelines are utilized during model fitting, selective augmentation techniques are available and configurable through the same files if needed.

\section{Bayesian Hyperparameter Optimization}
\label{sec:bayesian_optimization}

I employed \acrshort{wandb} Sweeps to perform Bayesian Optimization for efficient hyperparameter tuning. A Gaussian Process surrogate model was used to represent the validation \acrshort{map} as a function of learning rate, weight decay, and hidden dimension. Uniform and log-uniform priors were defined over each hyperparameter in the sweep configuration *.YAML file. The expected improvement acquisition function guided the selection of promising hyperparameter combinations. 70 trials were run in three iterations. The 70 runs were in parallel\footnote{All runs were run as individual training and had nothing to do with each other except the \acrshort{wandb} agent designing parameters. } across multiple GPUs to explore the search space while minimizing wall-clock time. All metrics and configuration parameters were logged to \acrshort{wandb}, enabling real-time monitoring and reproducibility. 

% probabilistic model of the objective function (e.g., validation \acrshort{map}) and using an acquisition function to decide which hyperparameters to evaluate next. This approach balances exploration (trying new, uncertain areas) and exploitation (focusing on areas that yield good results).


\section{\acrshort{vms} Evaluation Protocol}
When evaluating \acrshort{vms} performance, the 50\% \acrshort{map} criterion from the SoccerNet benchmark is adopted, which employs a $\pm1s$ tolerance window for event matching. Since the \acrshort{vms} model is validated against fixed‐length intervals of $L = 2\,$s, achieving $\mathrm{IoU}\ge0.5$ requires an overlap of at least $L/2 = 1s\,$.

Let $t^*$ denote the ground‐truth event time and $t_p$ the predicted center. The predicted interval is
\[
    [\,t_p - L/2,\;t_p + L/2\,]
    = [\,t_p - 1,\;t_p + 1\,].
\] 
The IoU condition
\[
    \mathrm{IoU}
    = \frac{\text{overlap}}{\text{union}}
    \;\ge0.5
    \quad\Longrightarrow\quad
    \text{overlap}\ge1
    \quad\Longrightarrow\quad
    |t_p - t^*|\le1
\]
ensures that the predicted center $t_p$ lies within one second of the actual event time.

I measured runtime directly from \acrshort{wandb} logs, as the platform automatically records wall-clock time for each training run. However, these values vary according to the assigned \acrshort{gpu} model. All measurements were standardized on a single \acrshort{gpu} type to ensure fair comparisons. 

\section{Environment} \todo 

Multiple environments
I used Python, but the environments also had CUDA code and C++ code. 
Are different repositories for different experiments
a VMS environment or a tool?
Can I just put environments in an appendix? Environments are stored in wandb, if I understand it correctly, with version control.