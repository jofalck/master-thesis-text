\chapter{Introduction}
\label{chap:intro}

The global sports industry is a major part of the entertainment sector. In 2023, sports betting alone generated over 242 billion USD in revenue (Statista, 2023). At the same time, top‐level football clubs and analytics firms invest heavily in data and technology to gain a competitive edge. For example, Liverpool FC has integrated advanced AI tools into their corner preparation \cite{wang_tactic_ai_2024}. In Norway, Bodø/Glimt together with \hyperlink{https://fokus.ing}{fokus.ing} has improved player recruitment through machine learning.

Despite these advances, most football annotation remains a manual, costly process handled by a few specialized companies. This limits clubs’ ability to build and own their own datasets—yet in today’s “data‐driven” world, exclusive access to high‐quality labels can create a decisive advantage. To stimulate progress, SoccerNet hosts annual challenges on tasks such as ball action spotting, providing public benchmarks and annotated video for the research community.

This thesis investigates whether recent methods from other areas of computer vision (\acrlong{cv}) – in particular masked autoencoders – can improve temporal action localization in football video. I propose a simple yet effective pipeline that extracts spatio‐temporal features from raw footage, and evaluate it against state‐of‐the‐art (\acrfull{sota}) baselines in terms of accuracy, runtime, and robustness to different action types.

\section{Motivation}
Football has entered a new era in which data-driven decision making is no longer a novelty but a necessity. With the global sports betting market alone accounting for over 242 \$ billion in revenue in 2023, clubs and analytics firms alike are under immense pressure to extract every possible competitive edge. Traditionally, on‐field performance data has been captured through GPS vests and manual video annotation—both of which have significant drawbacks. GPS vests only profile the players who wear them and remain unpopular among many teams, while manual annotation is slow, expensive, and often monopolized by a few vendors.  

Video data, by contrast, is abundant and impartial. Every match is filmed from multiple angles and stored indefinitely—presenting a vast, underutilized resource for extracting rich spatio‐temporal features. Automated video analysis can unlock insights into player positioning, tactical patterns, and event dynamics for both home and away teams, democratizing access to high‑quality data. This is critical in a field where “data is the new oil,” and owning proprietary datasets can translate directly into on‑pitch success and financial return.  

Competitions such as SoccerNet’s annual challenges have driven progress in temporal action spotting by providing standardized benchmarks and annotated corpora. Yet the state of the art still struggles with fine‐grained action localization under real‐world conditions: varied camera motions, crowded scenes, and subtle player interactions that define the beautiful game. Developing more robust, generalizable methods will not only advance the academic field of computer vision but also deliver practical tools for clubs, scouts, and broadcasters.  

This thesis is motivated by the opportunity to bridge advances in masked autoencoding and self‐supervised learning with the specific demands of football video analysis. By leveraging large video corpora without manual labels, I aim to (1) reduce annotation costs, (2) improve the accuracy and speed of action spotting, and (3) enable clubs of all sizes to harness video analytics for player recruitment, tactical preparation, and fan engagement.  

\section{Goals and Research Questions}

\begin{itemize}
    \item \textbf{RQ1:} How does the accuracy of the proposed masked-autoencoder and mamba temporal action spotting model compare to existing \acrlong{sota} methods on the SoccerNet benchmark?
    \item \textbf{RQ2:} What are the model’s inference speed and computational requirements under realistic operating conditions?
\end{itemize}

\section{Research Method}

The thesis designs an experimental design for a model capable of localizing temporal actions. The system extracts features from videos using masked autoencoding.

\section{Contributions}

\section{Thesis Outline}

