\chapter{Introduction}
\label{chap:intro}

The global sports industry is a significant part of the entertainment sector. In 2023, sports betting alone generated over 242 billion USD in revenue (Statista, 2023). At the same time, top-level football clubs and analytics firms invest heavily in data and technology to gain a competitive edge. For example, Liverpool FC has integrated advanced AI tools into their corner preparation \cite{wang_tactic_ai_2024}. In Norway, Bodø/Glimt, in collaboration with \hyperlink{https://fokus.ing}{fokus.ing}, has enhanced player recruitment using machine learning.

Despite these advances, most football annotations remain manual and costly processes handled by a few specialized companies. This limits clubs' ability to build and own their datasets—yet in today's "data-driven" world, exclusive access to high-quality labels can create a decisive advantage. SoccerNet hosts annual challenges on tasks such as ball action spotting to stimulate progress and provide public benchmarks and annotated videos for the research community.

This thesis investigates whether recent methods from other areas of computer vision (\acrlong{cv}) – specifically, masked autoencoders – can enhance temporal action localization in football videos. The thesis proposes a simple yet effective pipeline that extracts spatiotemporal features from raw footage and evaluates it against state-of-the-art (\acrfull{sota}) baselines regarding accuracy, runtime, and robustness to different action types.

\section{Motivation}
Football has entered a new era in which data-driven decision-making is no longer a novelty but a necessity. With the global sports betting market alone accounting for over 242 \$ billion in revenue in 2023, clubs and analytics firms are under immense pressure to extract every possible competitive edge. Traditionally, teams capture on-field performance data using GPS vests and manual video annotation, which have significant drawbacks. GPS vests only profile the players who wear them and remain unpopular among many teams, while manual annotation is slow, expensive, and often monopolized by a few vendors.  

Video data, by contrast, is abundant and impartial. Every match is filmed from multiple angles and stored indefinitely, presenting a vast, underutilized resource for extracting rich spatiotemporal features. Automated video analysis can unlock insights into player positioning, tactical patterns, and event dynamics. It can democratize access to high-quality data. High-quality data is critical in a world where "data is the new oil," and owning proprietary datasets can translate directly into on-pitch success and financial return.  

Competitions such as SoccerNet's annual challenges have driven progress in temporal action spotting by providing standardized benchmarks and annotated corpora. Nevertheless, the state of the art still struggles with fine-grained action localization under real-world conditions: varied camera motions, crowded scenes, and subtle player interactions that define the beautiful game. Developing more robust, generalizable methods will advance the academic field of computer vision and deliver practical tools for clubs, scouts, and broadcasters.  

This thesis is motivated by the opportunity to bridge advances in masked autoencoding and self-supervised learning with the specific demands of football video analysis. By leveraging large video corpora without manual labels, the aim is to (1) reduce annotation costs, (2) improve the accuracy and speed of action spotting, and (3) enable clubs of all sizes to harness video analytics for player recruitment, tactical preparation, and fan engagement.  

\section{Research Questions}
\label{sec:research_questions}
\begin{itemize}
    \item \textbf{RQ1:} How does the accuracy of the masked-autoencoder and mamba temporal action spotting model compare to existing \acrlong{sota} methods on the SoccerNet benchmark?
    \item \textbf{RQ2:} What are the model's inference speed and computational requirements compared to T-DEED?
    \item \textbf{RQ3:} Which types of adaptation can be made to the MAMBA model to increase its precision?
\end{itemize}

\section{Research Method}

The thesis presents an experimental design for a model capable of localizing temporal actions. The system extracts features from videos using masked autoencoding.

This thesis applies the \acrfull{vms} to a football video dataset to evaluate its event-spotting performance (see Chapter~\ref{chap:experiments}). The results demonstrate that \acrshort{vms} achieves not only competitive accuracy but also substantial gains in inference speed and memory efficiency, confirming the practicality of state-space models for large-scale video analytics. 

\subsection{Action Spotting vs.\ Action Localization}
Action spotting casts each event as a single instance, reducing annotation to a single timestamp per occurrence. By contrast, action localization requires predicting both start and end times for each action interval and is typically evaluated with temporal \acrfull{iou} metrics. Spotting simplifies supervision and evaluation (via tolerance windows around accurate timestamps), but it also challenges models to resolve events occurring in close temporal proximity without relying on segment boundaries.

\subsection{Temporal vs.\ Spatiotemporal Tasks}
Temporal event spotting focuses solely on "when" an action occurs, using global frame or clip-level features. Spatiotemporal tasks, in comparison, demand joint localization in time and space (e.g., \ frame-level bounding boxes). The SoccerNet challenge remains in the temporal domain.


\subsection{Temporal vs.\ Spatiotemporal Tasks}
Temporal event spotting focuses solely on "when" an action occurs, using global frame or clip-level features. Spatiotemporal tasks, in comparison, demand joint localization in time and space (e.g., \ frame-level bounding boxes). The SoccerNet challenge remains in the temporal domain.



\section{Contributions}

This thesis explores a novel utilization of the MAMBA \acrshort{s6} framework in the context of football. To the author's knowledge, as of June 2025, no efforts have been made in this field of study. 

\section{Thesis Outline}

